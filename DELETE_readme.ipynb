{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398a547c-37d7-4aa3-a78c-5268b1df5a06",
   "metadata": {},
   "source": [
    "\n",
    "## Project_5\n",
    "#### van Possum group - Jordan Gates, Joomart Achekeev, Brian Kim\n",
    "\n",
    "[Executive Summary](./exec_summary.md)\n",
    "\n",
    "### Intro\n",
    "Given a wide selection of topics to work on for the project 5, van Possum group (Joomart, Jordan and Brian) has selected the vital issue of unemployment. In an attempt to model a real life project the group members have decided to ignore the available datasets and collect raw, uneddited and uncleaned data from numerous sources. The collected data included maximum information possible on such topics as numer of police per capita, number of enterprises per capita, population breakdown per capita and other related topics. The data was collected by US county for 2010, excluding the 2008 presidential election results, that have been used to derive the democrat/republican percentage of local population.\n",
    "\n",
    "We set a very optimistic goal before actually touching the project to determine is it possible to predict unemployment rate depending on the certain characteristics of a standalone county?\n",
    "\n",
    "The project problem statement is as follows: What factors correlate with the unemployment and can they be used to predict the unemployment rate in a random US county? \n",
    "\n",
    "And as a consequence, by tweaking these factors would it be possible to influence the unemployment rate?\n",
    "\n",
    "##### (parts of the readme are in the exact order how the python codebooks are to be executed)\n",
    "\n",
    "### Used tools and Python libraries\n",
    "\n",
    "First code notebook, “01_Data_load_and_cleaning.ipynb” used libraries such as pandas and numpy for bringing in data and changing the types of the columns.\n",
    "The Second code notebook, “02_EDA.ipynb”, required pandas, numpy, seaborn, and matplotlib to bring in data and graphing the outcomes of our findings. \n",
    "Third notebook, “03_Preprocessing_and_Feature_engineering.ipynb”, used more libraries because it required us to train test split. Along with pandas, numpy, and matplotlib, we brought in various sklearn imports to perform initial imports. In sklearn imports, there were LinearRegression, StandardScalar, PolynomialFeatures, train_test_split, cross_val_score and predict, Pipeline, GridSearchCV and PCA for our feature modeling. Using these, we were able to break our data, scale them and analyze how our model performed with any given columns.\n",
    "Last notebook, “04_Modelling.ipynb”, we had the same ones as above in addition to the Tensorflow, keras libraries. With these, we performed neural network predictions to make our already existing model better. Through all the research, we were able to get a feasible model for our data. (edited) \n",
    "\n",
    "### 01 Data loading and cleaning\n",
    "\n",
    "Due to the fact that the data was extracted from various sources, around 4.2% of the data was so corrupted and was eveluated as impossible to use in the modelling, this data was droped. Other values have been imputed with average statewide data since the rows where they were located contained other important information that would help our models learn important dependencies. The final dataset prepared for futher processing has 28 columns of which 3 are location data (state, county, population) and 1 is the target data - unemployment.\n",
    "\n",
    "##### Main Data Sources:\n",
    "\n",
    "US Census (www.census.gov):\n",
    " - economic data\n",
    " - population data\n",
    " - unemployment data\n",
    "USDA (www.nass.usda.gov):\n",
    " - rural/urban population data\n",
    "Harvard University (www.dataverse.harvard.edu):\n",
    " - elections data\n",
    "FBI (ucr.fbi.gov)\n",
    " - police data\n",
    " - crimes data\n",
    "\n",
    "\n",
    "\n",
    "### 02 EDA\n",
    "Working with the Eda of our data, we were able to visually show what our findings looked like. For example, we first looked at the correlation of the whole data. With this, we were able to tell, which columns might have more impact towards our target variable ‘unemployment_rate_2010’. There are few graphs that we took a look at in more detail. \n",
    "\n",
    "Few scatter plots we have shown how the Democratic votes in 2008 graphed generally in a positive trend while Republican votes graphed in a negative trend. We have also graphed the relationship between population in different counties and the unemployment rate. But, inn general, it did not give sufficient evidence that population has much to do with unemployment rate. Same goes for the crimes per capita and unemployment rate. \n",
    "\n",
    "One surprising data we found was how much liquor stores per 10,000 people lead to the the high employment rate. The number of liquor stores per 10,000 people inn counties with unemployment rate less than 5% was 2.06 liquor stores per 10,000. While counties with unemployment rate more than 15% had 0.78 liquor stores per 10,000 people.\n",
    "\n",
    "### 03 Preprocessing and Feature Engineering\n",
    "\n",
    "This is the part where it got challenging and interesting. The original data showed low scores (train: 0.4299, test: 0.3938, cross_val: 0.3593) while fully polynomial transformed dataset showed extreame variance with over 600 columns. This is where the team had an idea to nad create grid search analog for the linear model in terms of correlation percentages of the numeric columns (see codebook 03 for more details). \n",
    "\n",
    "The model had nested double loop with sufficient code inside and took some time to run but yeilded better scores with indication of which column interaction will return better results in the linear regression model.\n",
    "\n",
    "The final dataset with certain data interaction had 101 columns and was save for future use in the modeling step. \n",
    "\n",
    "### 04 Modeling\n",
    "As the modelling process is faster then the previous steps where, the team decided to compare the basic Linear Regression and Neural Network Performance. Even though the amount of data was limited by 3015 rows, neural network performed significantly better.\n",
    "    Linear Regression returned Test 0.4836, Cross_val 0.4572\n",
    "    Neural Network returned Test 0.6344\n",
    "Even though there where other models such as Linear regression on the original not preprocessed dataset, and linear regression on PCA tuned dataset, but given poor performance of these models they were not used to train second level model.\n",
    "\n",
    "Second level Linear Regression model was hand fed train test split predictions from first level models and returned even better results Test 0.935, Cross_val 0.921 (the team was actually scared of the result and went through the whole code several times to eliminate the possible mistakes)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Given a task to select a project, find data, and pick a model to train is a very broad task. I believe that real world data scientist face this only when they are working on individual projects, the constraints that are provided by the employer, be that a company, or a research institution actually are greatly helpful in terms of the work flow, and let the Data Scientist focus on the work rather then searching for a question to research.\n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "| *Column Name* | *Description* |\n",
    "| :----------- | :----------- |\n",
    "| state | Name of State |\n",
    "| county_name | Name of County |\n",
    "| unemployment_rate_2010 | County Unemployment Rate in 2010|\n",
    "| urban_population_prc | County Urban Population % |\n",
    "| rural_population_prc | County Rural Population % |\n",
    "| crime_per_capita | County Crime Per Capita |\n",
    "| per_capita_sme_num |County small and medium enterprises per capita |\n",
    "| per_capita_large_num | County large enterprises per capita |\n",
    "| avg_ann_pay_per_emp_sme | County average annual pay per small/medium size employer |\n",
    "| avg_ann_pay_per_emp_large | County average annual pay per large size employer |\n",
    "| avg_ann_pay_per_emp_total | County average annual pay per employer |\n",
    "| population_jail_prc | County percentage of populatoin in jail |\n",
    "| 2008_dem_%_vote | County percentage of population that voted Democrat in 2008 |\n",
    "| 2008_rep_%_vote | County percentage of population that voted Republican in 2008 |\n",
    "| 2008_other_%_vote | County percentage of population that voted other in 2008 |\n",
    "| smoke_percent_2010 | County percentage of population that smokes |\n",
    "| popul_hs_grad_prc | County percentage of population that graduated high school |\n",
    "| popul_college_grad_prc | County percentage of population that graduated college |\n",
    "| popul_single_paren_prc | County percentage of population that are single parents |\n",
    "| liquor_stores_per10k | County number of liquor stores per 10,000 people |\n",
    "| police_per_1000 | County number of police per 1,000 people |\n",
    "| WhiteNonHispanicPct2010 | County percentage of white, non-hispanic people |\n",
    "| BlackNonHispanicPct2010 | County percentage of black, non-hispanic people |\n",
    "| AsianNonHispanicPct2010 |  County percentage of asian, non-hispanic people |\n",
    "| NativeAmericanNonHispanicPct2010 | County percentage of native american, non-hispanic people |\n",
    "| HispanicPct2010 | County percentage of hispanic people |\n",
    "| MultipleRacePct2010 | County percentage of multi race people |\n",
    "\n",
    "### Proper codebook order\n",
    "\n",
    "01 Data loading and cleaning\n",
    "02 EDA\n",
    "03 Preprocessing and feature engineering\n",
    "04 Modeling\n",
    "\n",
    "Actual code is located in the code subfolder of the project root directory\n",
    "\n",
    "### Additional resources\n",
    "\n",
    "##### Graphs\n",
    "graphs derived from the codebooks and saved for use in presentation\n",
    "\n",
    "##### Tableau\n",
    "graphs and maps derived from Tableau public for use in presentation\n",
    "\n",
    "##### Data\n",
    "actual folder of the initial dataset and the datasets that are pased by codebooks to the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2804f-d534-48df-9b80-2d527e7f7421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
